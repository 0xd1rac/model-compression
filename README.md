# Papers

## Pruning
- [ ] Model Compression and Hardware Acceleration for Neural Networks: A Comprehensive Survey [Deng et al., IEEE 2020]
- [ ] Learning Both Weights and Connections for E ffi cient Neural Network [Han et al., NeurIPS 2015]
- [ ] Optimal Brain Damage [LeCun et al., NeurIPS 1989]
- [ ] Learning Both Weights and Connections for E ffi cient Neural Network [Han et al., NeurIPS 2015]
- [ ] AMC: Automl for Model Compression and Acceleration on Mobile Devices [He et al., ECCV 2018]
- [ ] Network Trimming: A Data-Driven Neuron Pruning Approach towards E ffi cient Deep Architectures [Hu et al., ArXiv 2017]
- [ ] ThiNet: A Filter Level Pruning Method for Deep Neural Network Compression [Luo et al., ICCV 2017]
- [ ] SparseGPT: Massive Language Models Can be Accurately Pruned in One-Shot [Elias Frantar, Dan Alistarh, ArXiv 2023]
- [ ] Pruning Convolutional Filters with First Order Taylor Series Ranking [Wang M.]
- [ ] Importance Estimation for Neural Network Pruning [Molchanov et al., CVPR 2019]
- [ ] Learning Structured Sparsity in Deep Neural Networks [Wen et al., NeurIPS 2016]
- [ ] Exploring the granularity of sparsity in convolutional neural networks [Mao et al., CVPR-W]


## Quantization
- [ ] Deep Compression [Han et al., ICLR 2016]
- [ ] HAQ: Hardware-Aware Automated Quantization with Mixed Precision [Wang et al., CVPR 2019]
- [ ] Towards Accurate Binary Convolutional Neural Network [Lin et al., NeurIPS 2017]
- [ ] Binarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1. [Courbariaux et al., Arxiv 2016]
- [ ] Post-Training 4-Bit Quantization of Convolution Networks for Rapid-Deployment [Banner et al., NeurIPS 2019]
- [ ] PACT: Parameterized Clipping Activation for Quantized Neural Networks [Choi et al., arXiv 2018]
- [ ] Model Compression and Hardware Acceleration for Neural Networks: A Comprehensive Survey [Deng et al., IEEE 2020]
- [ ] BinaryConnect: Training Deep Neural Networks with Binary Weights during Propagations [Courbariaux et al., NeurIPS 2015]
- [ ] XNOR-Net: ImageNet Classification using Binary Convolutional Neural Networks [Rastegari etal.,ECCV 2016]
- [ ] Ternary Weight Networks [Li et al., Arxiv 2016]
- [ ] Trained Ternary Quantization [Zhu et al., ICLR 2017]

## Knowledge Distillation
- [ ] Network Augmentation for Tiny Deep Learning [Cai et al. , ICLR 2022]
- [ ] Distilling the Knowledge in a Neural Network [Hinton et al. , NeurIPS Workshops 2014]
- [ ] Knowledge Distillation: A Survey [Gou et al. , IJCV 2020]
- [ ] FitNets: Hints for Thin Deep Nets [Romero et al. , ICLR 2015]
- [ ] Knowledge Transfer via Distillation of Activation Boundaries Formed by Hidden Neurons [Heo et al. , AAAI 2019]
- [ ] Relational Knowledge Distillation [Park et al. , CVPR 2019]
- [ ] Born-Again Neural Networks [Furlanello et al., ICML 2018]
- [ ] Deep Mutual Learning [Zhang et al. , CVPR 2018]
- [ ] Knowledge Distillation by On-the-Fly Native Ensemble [Lan et al. , NeurIPS 2018]
- [ ] Be Your Own Teacher: Improve the Performance of Convolutional Neural Networks via Self Distillation [Zhang et al. , ICCV 2019]
- [ ] MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices [Sun et al. , ACL 2020]
- [ ] AutoAugment: Learning Augmentation Policies from Data [Cubuk et al. , CVPR 2019]


## Neural Architecture Search 
- [ ] Neural Architecture Search: Insights from 1000 Papers
- [ ] SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <0.5MB model size [Iandola et al., arXiv 2016]
- [ ] Neural Architecture Search: A Survey [Elskan et al., JMLR 2019]
- [ ] Learning Transferable Architectures for Scalable Image Recognition [Zoph et al., CVPR 2018]
- [ ] DARTS: Differentiable Architecture Search [Liu et al., ICLR 2019]
- [ ] MnasNet: Platform-Aware Neural Architecture Search for Mobile [Tan etal., CVPR 2019]
- [ ] ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware [Cai et al., ICLR 2019]
- [ ] FBNet: Hardware-Aware E ffi cient ConvNet Design via Di ff erentiable Neural Architecture Search [Wu et al., CVPR 2019]
- [ ] Once-for-All: Train One Network and Specialize it for E ffi cient Deployment [Cai et al. , ICLR 2020]
- [ ] MCUNet: Tiny Deep Learning on IoT Devices [Lin et al., NeurIPS 2020]
- [ ] Neural Architecture Search with Reinforcement Learning [Zoph and Le, ICLR 2017]
- [ ] Net2Net: Accelerating Learning via Knowledge Transfer [Chen et al., ICLR 2016]
- [ ] Efficient Architecture Search by Network Transformation [Cai et al., AAAI 2018]
- [ ] SMASH: One-Shot Model Architecture Search through HyperNetworks [Brock et al., ICLR 2018]
- [ ] Efficient Neural Architecture Search via Parameter Sharing [Pham et al., ICML 2018]
- [ ] GradSign: Model Performance Inference with Theoretical Insights [Zhang and Jia, ICLR 2022]
- [ ] Zen-NAS: A Zero-Shot NAS for High-Performance Image Recognition [Lin et al., ICCV 2021]
- [ ] SNIP: Single-Shot Network Pruning based on Connection Sensitivity [Lee et al., ICLR 2019]
- [ ] Flextron: Many-in-One Flexible Large Language Model [Cai et al., ICML 2024]

## LLM Deployment Techniques
- [ ] SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models
- [ ] AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration
- [ ] A Simple and Effective Pruning Approach for Large Language Models - WANDA
- [ ] Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time
- [ ] Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity
- [ ] A Review of Sparse Expert Models in Deep Learning
- [ ] SpAtten: Efficient Sparse Attention Architecture with Cascade Token and Head Pruning
- [ ] H2O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models
- [ ] Efficient Memory Management for Large Language Model Serving with PagedAttention
- [ ] FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness
- [ ] Fast Inference from Transformers via Speculative Decoding

## LLM Post-Training 
- [ ] BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models
- [ ] TinyTL: Reduce Activations, Not Trainable Parameters for Efficient On-Device Learning
- [ ] Parameter-Efficient Transfer Learning for NLP
- [ ] LoRA: Low-Rank Adaptation of Large Language Models
- [ ] The Power of Scale for Parameter-Efficient Prompt Tuning
- [ ] Prefix-Tuning: Optimizing Continuous Prompts for Generation
- [ ] QLoRA: Efficient Finetuning of Quantized LLMs
- [ ] BitDelta: Your Fine-Tune May Only Be Worth One Bit
- [ ] Finetuned Language Models Are Zero-Shot Learners
- [ ] Emergent Abilities of Large Language Models
- [ ] Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks

## Long Context LLM
- [ ] Focused Transformer: Contrastive Training for Context Scaling
- [ ] LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models
- [ ] Lost in the Middle: How Language Models Use Long Contexts
- [ ] LongBench: A Bilingual, Multitask Benchmark for Long Context Understanding
      
