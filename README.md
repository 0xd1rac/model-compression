# Papers

## Pruning

## Quantization

## Knowledge Distillation
- [ ] Deep Compression [Han et al., ICLR 2016]
- [ ] HAQ: Hardware-Aware Automated Quantization with Mixed Precision [Wang et al., CVPR 2019]
- [ ] Towards Accurate Binary Convolutional Neural Network [Lin et al., NeurIPS 2017]
- [ ] Binarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1. [Courbariaux et al., Arxiv 2016]
- [ ] Post-Training 4-Bit Quantization of Convolution Networks for Rapid-Deployment [Banner et al., NeurIPS 2019]
- [ ] PACT: Parameterized Clipping Activation for Quantized Neural Networks [Choi et al., arXiv 2018]
- [ ] Model Compression and Hardware Acceleration for Neural Networks: A Comprehensive Survey [Deng et al., IEEE 2020]
- [ ] BinaryConnect: Training Deep Neural Networks with Binary Weights during Propagations [Courbariaux et al., NeurIPS 2015]
- [ ] XNOR-Net: ImageNet Classification using Binary Convolutional Neural Networks [Rastegari etal.,ECCV 2016]
- [ ] 


## Neural Architecture Search 
